<!DOCTYPE html>
<html>
<head>
<meta name="viewport" content="width=device-width, initial-scale=1">
<meta charset="utf-8">
<link href="https://fonts.googleapis.com/css?family=Assistant:200&display=swap" rel="stylesheet">

<title>Latent Space of My Mind</title>
<link rel="apple-touch-icon" sizes="180x180" href="apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="favicon-16x16.png">
<link rel="manifest" href="site.webmanifest">
<link rel="mask-icon" href="safari-pinned-tab.svg" color="#c85b00">
<meta name="apple-mobile-web-app-title" content="Soroush">
<meta name="application-name" content="Soroush">
<meta name="msapplication-TileColor" content="#2b5797">
<meta name="theme-color" content="#fff4e8">



<style type="text/css">
	body
	{
		
		width:100%;
		text-align: center;
		font-family: "HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
		font-size:16px;
		background-color: #fff4e8;
		margin: 0;
		padding: 0;
	}
	hr
	{
		border: 0;
		height: 2px;
		background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	}

	.gallery {
	    width: 80%;
	    max-width: 1200px; /* Limits the maximum width to ensure the gallery isn't too wide on large screens */
	    margin: 20px auto; /* Centers the gallery on the page with margin top and bottom */
	    padding: 20px;
	    background: #fff4e8; /* White background for the gallery */
	}

	.art-piece {
	    margin-bottom: 20px; /* Space between art pieces */
	}
	
	.art-piece img {
	    width: 100%; /* Makes the image responsive */
	    height: auto; /* Maintains aspect ratio */
	}
	
	.art-piece p {
	    margin: 10px 0; /* Vertical margin for the text, no horizontal margin */
	}

	
	#info-writing
	{
	  width:80%;
	  margin: 10%;
	  padding: 0px ;
	  margin-bottom: 50% ;
	  background-color: rgba(200, 200, 200, 0);
	  z-index: 1;
	}
	div.sticky
	{
	  position: -webkit-sticky;
	  position: sticky;
	  top: 0 ;
	  z-index: -1 ;
	}

	#canvas-container
	{
		margin:0 ;
		z-index: -1;

	}

	table
	{
		padding: 5px;
	}
	
	
	#info-bio
	{
	  margin: 5%;
	  padding: 0px ;
	  height: 700px;
	  display: inline-block;
	}


	table.pub_table,td.pub_td1,td.pub_td2
	{
		border-collapse: collapse;
		border-bottom: 0px solid #9B9B9B;
		padding-bottom: 10px;
		padding-top: 10px;
		padding-left: 10px;
		width: 100%;
		display: inline-table;
		margin-bottom: 50px;
	}
	
	table.pub_table2
	{
		border-collapse: collapse;
		border-bottom: 0px solid #9B9B9B;
		padding-bottom: 10px;
		padding-top: 10px;
		padding-left: 10px;
		width: 100%;
		display: inline-table;
		margin-bottom: 50px;
		min-width: 200px;
		max-width: 700px;
	}
	
	
	td.pub_td1
	{
		width: 40%;
		min-width: 300px;
		max-width: 700px;
	}
	td.pub_td2
	{
		width: 50%;
		min-width: 300px;
	}
	td.pub_td3
	{
		width: 200px;
	}
	
	td.year_heading
	{
		color: #3B3B3B;
		font-weight: 700;
		font-size:20px;
	}
	tr {
		display: table; 
		width: 100%
	}

	div#container
	{
		margin-left: auto;
		margin-right: auto;
		width: 1200px;
		text-align: left;
		position: relative;
	}
	div#DocInfo
	{
		color: #9B9B9B;
		height: 60%;
		font-size: 80%;
	}
	h4,h3,h2,h1
	{
		color: #3B3B3B;
	}
	h2
	{
		font-size:130%;
	}
	p
	{
		color: #000;
		margin-bottom: 20px;
	}
	p.caption
	{
		color: #9B9B9B;
		text-align: left;
		width: 600px;
		font:11px helvetica,sans-serif;
	}
	p.caption2
	{
		color: #9B9B9B;
		text-align: left;
		width: 800px;
		font:11px helvetica,sans-serif;
	}
	#header_img
	{
		position: absolute;
		top: 0px; right: 0px;
	}
	a:link,a:visited
	{
		color: #1367a7;
		text-decoration: none;
	}
	.section_div {
		padding: 10px 10px 10px 10px;
		margin: 10px 10px 10px 10px;
		//border: 1px solid #AAA;
	}
	body {
	}
	#personal_info {
		width: 100%;
		display: initial;
	}
	p.announcement {
		padding: 10px;
		background-color: #EEE;
	}
	img.teaser_img {
		width: 100%;
		display: block;
		margin-left: auto;
		margin-right: auto;
		margin-top: 5px;
		margin-bottom: 5px;
		border: 0px solid black
		box-shadow: 0 0 12px 12px white inset;
	}
	
	img.logo {
		width: 100%;
		display: block;
		margin-left: auto;
		margin-right: auto;
		margin-top: 5px;
		margin-bottom: 5px;
		border: 0px solid black
		box-shadow: 0 0 12px 12px white inset;
	}
	
	img.photo_of_me {
		border-radius: 0px;
		width: 70%;
		max-width: 200px;
		min-width: 100px;
	}
	img.my_img {
		width: 50%;
		float: right;
		min-width: 300px;
		max-width: 500px; 
	}
	
	div.logo_div{
		width: 50%;
		display: inline-block;
		max-width: 200px;
		min-width: 150px;
	}
	
	div.contact_logo_div{
		width: 70%;
		display: inline-block;
		max-width: 150px;
	}
	
	div.teaser_img_div {
		width: 100%;
		box-shadow: 1px 1px 11px 0px grey;
	}
	table.personnel td {
		padding: 16px;
		vertical-align: top
	}

	canvas{width:100%;height:100%; background-color: #fff4e8;}
</style>

</head>



<body onload="main()">

	<table id="personal_info">

	
	
	<tr>
	<td><img class="photo_of_me" src="https://raw.githubusercontent.com/soroush-abbasi/soroush-abbasi.github.io/main/img_s.png" width=180px style=" float:left; margin-right:15px"/></td>
	<td><br>
	
	<div id="DocInfo">
		<h1>Soroush Abbasi Koohpayegani</h1>
	</div><br>
	</td>
	</tr>

	</table><hr>
	


  <div id="info-bio">



      <h2> Latent Space of My Mind </h2><hr style="margin-bottom:50px">
        <h4 style="float:left; width: 100%; max-width:820px;  margin-bottom:150px ; text-align:justify">
		Welcome to the Latent Space of My Mind! In my research, I've learned to communicate accurately. For example, the introduction should avoid overclaiming, 
		ensuring that every statement is supported by citations, empirical experiments, or theoretical justifications. 
		However, this contrasts with my mind, where I desire to speak and think freely without any constraints. Many of the contents presented here stem from my imagination rather than concrete facts. 
		
		In fact, I believe my strongest skill is my imagination. So, on this page, I won't concern myself with the correctness of my statements, and probably they are wrong.
		I want to speak freely and purely from the latent state of my mind. Feel free to email me and discuss your opinions—I love that. 
		
		I use GPT-4 to capture images from my mind. This process isn't easy, not because GPT-4 is dumb, but rather because I'm not smart enough to effectively communicate with it.
		Nonetheless, I find it to be the best solution. There's no one else with whom I can freely converse and ask to visualize my mind for me.
		So, respect to one of my best friends on this journey who understands me—the GPT! 
	</h4>

	  

   </div> 
   

	
	

<div class="gallery">
	<h2> The Dance Between High Entropy and Low Entropy  </h2><hr style="margin-bottom:50px">
        <div class="art-piece">
            <img src="https://soroush-abbasi.github.io/gallary/images/entropy_poster_thumb.jpg" alt="Artwork 1">
            <p style="width: auto;  margin-left: 0px; margin-right: 0px; margin-top: 30px; margin-bottom:150px ; text-align:justify">
		    I named this one "Entropy". It resembles a tree in nature or has a structure similar to UNet in neural networks.
		    UNets have been utilized in various models such as auto-encoders, where the objective is to represent any distribution in a compressed distribution 
		    (latent space) with a lower dimension. In recent diffusion models, UNet has been employed to progressively transfer the distribution of noise to natural images.
		    We can observe that distribution transfer occurs in nature as well. For instance, trees transport elements from the soil to form 
		    complex structures like leaves and fruits. 
<!-- 		    Here is my definition of entropy for a state. Low entropy states are states which are less likely to occur in the universe.
		    High entropy states are more likely to occur in the universe. -->
		    Nature consistently shifts distributions from higher entropy to lower entropy states. 
		    <br><br>
		    
		    This gradual change is similar to diffusion models, where each iteration only partially denoises the image. In our world, 
		    the distribution of specific states is intricately linked to the previous distribution. For instance, the presence of life depends 
		    on the existence of oxygen in the preceding distribution. Consequently, the probability of life increases with the presence of oxygen in the previous state.
		    
		    <br><br>

		    In the beginning, entropy is at its highest—pure random noise, with nothing known. 
		    What does the picture look like? Just a boring noise. At the end, entropy is at its lowest, where everything is known,
		    presenting a clear picture devoid of any room for imagination. It will remain the same boring picture from that point onwards. 
		    Somewhere in between, where entropy is balanced, lies the middle ground of the dance between high entropy and low entropy—an exciting distribution to live in. 
		    It's predictable enough to maintain stability, yet unpredictable enough to keep things exciting.
		    This represents a balance between chaos and order. These two states mirror each other, both being boring but in two different distributions.
			    <br><br>

		    Some additional details: I draw inspiration from the beauty of Turquoise, and this amazing video by
		    <a href="https://www.youtube.com/watch?v=DxL2HoqLbyA." style="color: hotpink"> Derek Alexander Muller</a>.
			    Dowload the 
		    <a href="https://soroush-abbasi.github.io/gallary/images/entropy_poster_web.jpg" style="color: hotpink"> high resolution</a>.
	</p>
        </div>





	<h2> The Hourglass  </h2><hr style="margin-bottom:50px">
        <div class="art-piece">
            <img src="https://soroush-abbasi.github.io/gallary/images/hour_glass.jpg" alt="Artwork 2">
            <p style="width: auto;  margin-left: 0px; margin-right: 0px; margin-top: 30px; margin-bottom:150px ; text-align:justify">
			For me, machine learning involves the transfer of data distributions. At times, our focus lies in sampling from distributions, 
		    while other times, we aim to transition from one distribution to another with desired properties. 
		    For instance, in discriminative tasks like image classification, our goal is to map the distribution of natural images to a compact distribution where images 
		    can be discriminated based on specific criteria. Each layer of a transformer network can represent a step that transforms the input 
		    distribution into another distribution until reaching the final layer, where the output originates from the target distribution. 
		    In generative models, our interest lies in sampling from specific distributions. The difficulty lies in the fact that not all distributions are equally probable to sample from.
		    States with low entropy are rare, making them challenging to sample. Conversely, states with high entropy are common and have 
		    a high probability of existence, making them easy to sample. 		    

		    <br><br>
		    
		    There is an asymmetry in the difficulty between constructing something and its destruction. 		    
		    Transitioning from a distribution with low entropy to one with high 
		    entropy is straightforward. A random walk on states can increase the entropy, and since high entropy distributions are 
		    more likely to occur, we eventually end up in those distributions after enough steps. Most of the distributions that we are 
		    interested in have low entropy and are rare. Examples include the distribution of natural images, the weights of a neural network 
		    that provide local minima of a function, or the solutions to a jigsaw puzzle. To sample from these rare distributions, we need to model the distribution.

		    <br><br>

		    If we could reverse time for the destruction process, we could sample a from a known distribution with high entropy (e.g., guassian noise) and then reverse time to return to the original distribution. 
		    However, since we don't have access to the function that performs the destruction, we cannot reverse time using the inverse of that function. 
		    Nevertheless, we might be able to learn the reverse process by parameterizing that function with a deep neural network. 
		    This entails learning to reverse time. 

		    <br><br>
		    
		    To learn the reverse process, we require data. We can sample from the target distribution and elevate the entropy within them through
		    destructive steps. With a carefully designed distructive steps, after a few iterations, this process transitions the distribution to known distribution with a higher probability of existence. 
		    Subsequently, we aim to learn to reverse this process—a function that, when given a high entropy distribution, transfers it to a 
		    lower entropy distribution, where it is more likely to sample instances within the target distribution. The critical aspect here is the ability to sample from the target distribution.

		    <br><br>
		    In general, I'm deeply intrigued by discovering all artifacts with the lowest entropy in the universe and model the distribution.
		    To appreciate the remarkable nature of a low entropy states, consider the probability of you, as a human, 
		    existing and reading this webpage on your computer, given the initial state of the universe—a universe with maximum
		    entropy and pure randomness of basis elements.

		    <br><br>

		    The hourglass, is similar to UNet, serves as a destruction function. The sands on the top part can originate from any distribution.
		    Once a sample from the target distribution is placed on top and the function is executed, it transforms any distribution into 
		    a normal-like distribution on the bottom part. I wish I could sample from the universe and place it atop the hourglass and learn the reverse function.
		    

		    
		</p>
        </div>

</div>

  
 



<div class='section_div' id='Papers' style="margin-top:200px">

 <h2>Contacts</h2><hr>

	<table class="pub_table" style="width: 90%">


	<tr>
	 <td class="pub_td3"><div class="contact_logo_div"><a href="https://www.youtube.com/channel/UCL-E2c0R_cuGuigA6F76Uqg"><img class="logo" src='/images/youtube.png'/></a></div></td>
	 <td class="pub_td3"><div class="contact_logo_div"><a href="https://soroush-abbasi.github.io"><img class="logo" src='/images/Dr.mac.png'/></a></div></td>
	 <td class="pub_td3"><div class="contact_logo_div"><a href="https://www.linkedin.com/in/soroush-abbasi-koohpayegani-5a4128ba/"><img class="logo" src='/images/linkedin.png'/></a></div></td>
	</tr>

	</table>

</div>




</body>
</html>
